{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tl_finetune_patch_camelyon.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOkDmEwOwoc19iyziubWjox",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AllergictoCrustaceans/ML/blob/main/tl_finetune_patch_camelyon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxzSUJzrNsj4"
      },
      "source": [
        "# Detect Metastatic Tissue \r\n",
        "\r\n",
        "Prompt: Given histopathologic scans of lymph node sections, classify whether a particular image has the presence of metastatic tissue. \r\n",
        "\r\n",
        "Where is data from: https://patchcamelyon.grand-challenge.org/\r\n",
        "\r\n",
        "ML Type: CNN, Image Classification, Transfer Learning, Fine tuning\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "***\r\n",
        "\r\n",
        "NOTE: \r\n",
        "\r\n",
        "This ML project is particularly long to train, given that the dataset has over 300k data samples. This dataset may not be ideal to train under free Google Colab conditions. However, I will still upload my code, even though I have no idea how well this model is set up to learn. \r\n",
        "\r\n",
        "Meanwhile, I will find a solution. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDoigftoIwBH"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, LeakyReLU, Dropout\r\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Resizing, Rescaling\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "import zipfile\r\n",
        "import requests\r\n",
        "import io"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxKmGN3GK0JM"
      },
      "source": [
        "## Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xR_XTRpIxLq"
      },
      "source": [
        "(train_ds, test_ds, val_ds), info = tfds.load('patch_camelyon',\r\n",
        "                     split=['train', 'test', 'validation'],\r\n",
        "                     shuffle_files=True,\r\n",
        "                     as_supervised=True,\r\n",
        "                     with_info=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8bIGnyKLAZm"
      },
      "source": [
        "## Inspect"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8w5ElwOiI3OM"
      },
      "source": [
        "num_classes = info.features['label'].num_classes\r\n",
        "print(num_classes)\r\n",
        "\r\n",
        "fig = tfds.show_examples(train_ds, info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdNPL2hfLUT0"
      },
      "source": [
        "## Clean\r\n",
        "\r\n",
        "Nothing to clean. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zy8u--EGK3eD"
      },
      "source": [
        "## Split\r\n",
        "\r\n",
        "It's already split from tfds.load(...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2paMjoA6LXOz"
      },
      "source": [
        "# checked tensor shapes from info."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5MZiKFeLX5b"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwTdJqPLLfFq"
      },
      "source": [
        "IMG_SIZE=96\r\n",
        "\r\n",
        "resize_rescale = tf.keras.Sequential([\r\n",
        "                 Resizing(IMG_SIZE, IMG_SIZE),\r\n",
        "                 Rescaling(1./255.0)                     \r\n",
        "])\r\n",
        "\r\n",
        "data_augmentation = tf.keras.Sequential([\r\n",
        "                                         tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal_and_vertical'),\r\n",
        "                                         tf.keras.layers.experimental.preprocessing.RandomRotation(0.2)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaMZKY5OLZu8"
      },
      "source": [
        "# What's important about the order of cache, batch, and prefetch?\r\n",
        "\r\n",
        "batch_size=128\r\n",
        "AUTOTUNE=tf.data.AUTOTUNE\r\n",
        "\r\n",
        "# Build training pipeline\r\n",
        "def prepare(ds, shuffle=False, augment=False):\r\n",
        "    ds = ds.map(lambda x, y: (resize_rescale(x), y),\r\n",
        "                num_parallel_calls=AUTOTUNE)\r\n",
        "    \r\n",
        "    if shuffle:\r\n",
        "        ds = ds.shuffle(1000)\r\n",
        "    \r\n",
        "    ds = ds.batch(batch_size)\r\n",
        "\r\n",
        "    if augment:\r\n",
        "        ds = ds.map(lambda x, y: (data_augmentation(x), y),\r\n",
        "                    num_parallel_calls=AUTOTUNE)\r\n",
        "    \r\n",
        "    return ds.prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PASlcruUykrV"
      },
      "source": [
        "train_ds = prepare(train_ds, shuffle=True, augment=True)\r\n",
        "test_ds = prepare(test_ds)\r\n",
        "val_ds = prepare(val_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOBRW5OqJY1p"
      },
      "source": [
        "# REMINDER: Since this is image classification, you don't need to one-hot NOTHING."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fsSgWk7LaXb"
      },
      "source": [
        "## Create Plain Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwgFfzYTqfSx"
      },
      "source": [
        "METRICS = [\r\n",
        "           tf.keras.metrics.TruePositives(name='tp'),\r\n",
        "           tf.keras.metrics.TrueNegatives(name='tn'),\r\n",
        "           tf.keras.metrics.FalsePositives(name='fp'),\r\n",
        "           tf.keras.metrics.FalseNegatives(name='fn'),\r\n",
        "           tf.keras.metrics.BinaryAccuracy(name='accuracy'),\r\n",
        "           tf.keras.metrics.Precision(name='precision'),\r\n",
        "           tf.keras.metrics.Recall(name='recall'),\r\n",
        "           tf.keras.metrics.AUC(name='AUC')\r\n",
        "]\r\n",
        "\r\n",
        "def build(metrics=METRICS):\r\n",
        "    model = tf.keras.Sequential([\r\n",
        "                                 Conv2D(16, 3, padding='same',\r\n",
        "                                        activation=LeakyReLU(alpha=0.3),\r\n",
        "                                        input_shape=(96, 96, 3)),\r\n",
        "                                MaxPooling2D(),\r\n",
        "                                Conv2D(64, 3, padding='same',\r\n",
        "                                        activation=LeakyReLU(alpha=0.3)),\r\n",
        "                                MaxPooling2D(),\r\n",
        "                                Conv2D(128, 3, padding='same',\r\n",
        "                                        activation=LeakyReLU(alpha=0.3)),\r\n",
        "                                MaxPooling2D(),\r\n",
        "                                Flatten(),\r\n",
        "                                Dense(256, activation=LeakyReLU(alpha=0.3)),\r\n",
        "                                Dropout(0.2),\r\n",
        "                                Dense(1, activation='sigmoid')\r\n",
        "    ])\r\n",
        "\r\n",
        "    model.compile(\r\n",
        "        optimizer='adam',\r\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\r\n",
        "        metrics=metrics\r\n",
        "    )\r\n",
        "\r\n",
        "    return model\r\n",
        "\r\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\r\n",
        "    monitor='val_AUC',\r\n",
        "    verbose=1,\r\n",
        "    patience=10,\r\n",
        "    mode='max',\r\n",
        "    restore_best_weights=True\r\n",
        ")\r\n",
        "\r\n",
        "epochs = 10\r\n",
        "batch_size = 128\r\n",
        "\r\n",
        "plain_model = build()\r\n",
        "plain_history = plain_model.fit(\r\n",
        "    train_ds,\r\n",
        "    epochs=epochs,\r\n",
        "    batch_size=batch_size,\r\n",
        "    callbacks=[early_stopping],\r\n",
        "    validation_data=val_ds,\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIVAqe65zYl9"
      },
      "source": [
        "loss, acc = model.evaluate(test_ds)\r\n",
        "print(\"Accuracy\", acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TAB0o5izqQs"
      },
      "source": [
        "### Plots and Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxvnQl-ffP_9"
      },
      "source": [
        "acc = plain_history.history['accuracy']\r\n",
        "val_acc = plain_history.history['val_accuracy']\r\n",
        "\r\n",
        "loss = plain_history.history['loss']\r\n",
        "val_loss = plain_history.history['val_loss']\r\n",
        "\r\n",
        "plt.figure(figsize=(8, 8))\r\n",
        "plt.subplot(2, 1, 1)\r\n",
        "plt.plot(acc, label='Training Accuracy')\r\n",
        "plt.plot(val_acc, label='Validation Accuracy')\r\n",
        "plt.legend(loc='lower right')\r\n",
        "plt.ylabel('Accuracy')\r\n",
        "plt.ylim([min(plt.ylim()),1])\r\n",
        "plt.title('Training and Validation Accuracy')\r\n",
        "\r\n",
        "plt.subplot(2, 1, 2)\r\n",
        "plt.plot(loss, label='Training Loss')\r\n",
        "plt.plot(val_loss, label='Validation Loss')\r\n",
        "plt.legend(loc='upper right')\r\n",
        "plt.ylabel('Cross Entropy')\r\n",
        "plt.ylim([0,1.0])\r\n",
        "plt.title('Training and Validation Loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PccxOARHf6PL"
      },
      "source": [
        "There's a high bias problem. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vq895xPzZ1k"
      },
      "source": [
        "### With Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4Dgsq-Rzc-N"
      },
      "source": [
        "IMG_SHAPE= (96, 96, 3)\r\n",
        "tl_model = tf.keras.applications.ResNet50(\r\n",
        "    input_shape=IMG_SHAPE,\r\n",
        "    include_top=False,\r\n",
        "    weights='imagenet'\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-MUcM-M3Wia"
      },
      "source": [
        "image_batch, label_batch = next(iter(train_ds))\r\n",
        "feature_batch = tl_model(image_batch)\r\n",
        "print(feature_batch.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dtml24C-3lhr"
      },
      "source": [
        "tl_model.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCEvUA763pPC"
      },
      "source": [
        "tl_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn6srthN5Ddz"
      },
      "source": [
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\r\n",
        "feature_batch_average = global_average_layer(feature_batch)\r\n",
        "print(feature_batch_average.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IlzjWnj5EXZ"
      },
      "source": [
        "prediction_layer = tf.keras.layers.Dense(1)\r\n",
        "prediction_batch = prediction_layer(feature_batch_average)\r\n",
        "print(prediction_batch.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1L6r0nEF5ItH"
      },
      "source": [
        "preprocess_input = tf.keras.applications.resnet50.preprocess_input\r\n",
        "inputs = tf.keras.Input(shape=(96, 96, 3))\r\n",
        "x = data_augmentation(inputs)\r\n",
        "x = preprocess_input(x)\r\n",
        "x = tl_model(x, training=False)\r\n",
        "x = global_average_layer(x)\r\n",
        "x = tf.keras.layers.Dropout(0.2)(x)\r\n",
        "outputs = prediction_layer(x)\r\n",
        "\r\n",
        "model = tf.keras.Model(inputs, outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiMq50eU5Kl5"
      },
      "source": [
        "METRICS = [\r\n",
        "           tf.keras.metrics.TruePositives(name='tp'),\r\n",
        "           tf.keras.metrics.TrueNegatives(name='tn'),\r\n",
        "           tf.keras.metrics.FalsePositives(name='fp'),\r\n",
        "           tf.keras.metrics.FalseNegatives(name='fn'),\r\n",
        "           tf.keras.metrics.BinaryAccuracy(name='accuracy'),\r\n",
        "           tf.keras.metrics.Precision(name='precision'),\r\n",
        "           tf.keras.metrics.Recall(name='recall'),\r\n",
        "           tf.keras.metrics.AUC(name='AUC')\r\n",
        "]\r\n",
        "\r\n",
        "base_learning_rate = 0.0001\r\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\r\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SGAySv15anz"
      },
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(\r\n",
        "    monitor='val_AUC',\r\n",
        "    verbose=1,\r\n",
        "    patience=10,\r\n",
        "    mode='max',\r\n",
        "    restore_best_weights=True\r\n",
        ")\r\n",
        "\r\n",
        "epoch=10\r\n",
        "\r\n",
        "history = model.fit(train_ds,\r\n",
        "                    epochs=epoch,\r\n",
        "                    callbacks=[early_stopping],\r\n",
        "                    validation_data=(val_ds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy8gF04Y5vsS"
      },
      "source": [
        "### Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSxZaBpO5x8J"
      },
      "source": [
        "tl_model.trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ly28xCOWKju"
      },
      "source": [
        "# Let's take a look to see how many layers are in the base model\r\n",
        "print(\"Number of layers in the base model: \", len(tl_model.layers))\r\n",
        "\r\n",
        "# Fine-tune from this layer onwards\r\n",
        "fine_tune_at = 100\r\n",
        "\r\n",
        "# Freeze all the layers before the `fine_tune_at` layer\r\n",
        "for layer in tl_model.layers[:fine_tune_at]:\r\n",
        "  layer.trainable =  False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XykRlWRLWNnM"
      },
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\r\n",
        "              optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate/10),\r\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFp66yuCWOM0"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP3i6cPIWQUs"
      },
      "source": [
        "len(model.trainable_variables)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-tRbBkoWSdz"
      },
      "source": [
        "fine_tune_epochs = 10\r\n",
        "initial_epochs = 10\r\n",
        "total_epochs =  initial_epochs + fine_tune_epochs\r\n",
        "\r\n",
        "history_fine = model.fit(train_ds,\r\n",
        "                         epochs=total_epochs,\r\n",
        "                         initial_epoch=history.epoch[-1],\r\n",
        "                         validation_data=val_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRItUWhpWpxl"
      },
      "source": [
        "acc += history_fine.history['accuracy']\r\n",
        "val_acc += history_fine.history['val_accuracy']\r\n",
        "\r\n",
        "loss += history_fine.history['loss']\r\n",
        "val_loss += history_fine.history['val_loss']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q7KBbdzWqXc"
      },
      "source": [
        "plt.figure(figsize=(8, 8))\r\n",
        "plt.subplot(2, 1, 1)\r\n",
        "plt.plot(acc, label='Training Accuracy')\r\n",
        "plt.plot(val_acc, label='Validation Accuracy')\r\n",
        "plt.ylim([0.8, 1])\r\n",
        "plt.plot([initial_epochs-1,initial_epochs-1],\r\n",
        "          plt.ylim(), label='Start Fine Tuning')\r\n",
        "plt.legend(loc='lower right')\r\n",
        "plt.title('Training and Validation Accuracy')\r\n",
        "\r\n",
        "plt.subplot(2, 1, 2)\r\n",
        "plt.plot(loss, label='Training Loss')\r\n",
        "plt.plot(val_loss, label='Validation Loss')\r\n",
        "plt.ylim([0, 1.0])\r\n",
        "plt.plot([initial_epochs-1,initial_epochs-1],\r\n",
        "         plt.ylim(), label='Start Fine Tuning')\r\n",
        "plt.legend(loc='upper right')\r\n",
        "plt.title('Training and Validation Loss')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gygrcCbOWt-c"
      },
      "source": [
        "loss, accuracy = model.evaluate(test_dataset)\r\n",
        "print('Test accuracy :', accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}